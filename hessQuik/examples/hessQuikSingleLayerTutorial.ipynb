{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hessQuikSingleLayerTutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding ```hessQuik``` Layers\n",
        "\n",
        "In this notebook, we construct a slightly simplified version of a ```hessQuik``` single layer, show various methods of computing the gradient and Hessian of the network in forward mode, and time the various methods of computation.  This notebook should serve as a tutorial for constructing new layers, our methods of testing layers, and exploration of how to make implementations more efficient on CPUs and GPUs."
      ],
      "metadata": {
        "id": "zZ0IUeEe5YwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install ```hessQuik``` and Other Packages\n",
        "\n",
        "Here is also were you can select your data type and device.  We recommend testing the layer derivatives on a CPU with double precision first."
      ],
      "metadata": {
        "id": "shY0uPQr6O-B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZvLMZmwq2sW"
      },
      "outputs": [],
      "source": [
        "!python -m pip install git+https://github.com/elizabethnewman/hessQuik.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import hessQuik.activations as act\n",
        "import hessQuik.layers as lay\n",
        "\n",
        "# set precision\n",
        "torch.set_default_dtype(torch.float64)\n",
        "print('Default data type:', torch.get_default_dtype())\n",
        "\n",
        "# use GPU if available\n",
        "device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)"
      ],
      "metadata": {
        "id": "QnUPMw744syX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing Our First Layer\n",
        "\n",
        "Here, we show a slightly simplified single layer implementation where we remove the bias and do not use PyTorch's initialization of the layer parameters.  We also only implement a forward method for illustrative purposes.  You can find our full implementation here: [singleLayer](https://github.com/elizabethnewman/hessQuik/blob/main/hessQuik/layers/single_layer.py).\n",
        "\n",
        "<!-- The basics of this layer are the following. Given an input $\\mathbf{x}\\in \\mathbb{R}^{d}$ where $d$ is the dimension of the input,  suppose we have forward propagated using a function $u:\\mathbb{R}^d \\to \\mathbb{R}^n$.  \n",
        "Then, our current layer has the following dependence on $\\mathbf{x}$:\n",
        "\\begin{align}\n",
        "f(\\mathbf{x}) &= \\sigma(\\mathbf{K} u(\\mathbf{x})) && f:\\mathbb{R}^d \\to \\mathbb{R}^m \\text{ and } \\mathbf{K}\\in \\mathbb{R}^{m\\times n},\n",
        "\\end{align}\n",
        "and $\\sigma:\\mathbb{R}\\to \\mathbb{R}$ is a twice continuously differentiable activation function applied entrywise.\n",
        "\n",
        "The gradient of $f$ with respect to $\\mathbf{x}$ is given by\n",
        "\\begin{align}\n",
        "\\frac{\\partial f}{\\partial \\mathbf{x}}\n",
        "  &=\\frac{\\partial u}{\\partial \\mathbf{x}} \\frac{\\partial f}{\\partial u}\n",
        "  =\\frac{\\partial u}{\\partial \\mathbf{x}} \\mathbf{K}^\\top \\text{diag}(\\sigma'(\\mathbf{K} u(\\mathbf{x})))\n",
        "\\end{align}\n",
        "where $\\dfrac{\\partial u}{\\partial \\mathbf{x}}\\in \\mathbb{R}^{d\\times n}$ has been pre-computed.  Here, $\\sigma'$ is the derivative of the activation function applied entrywise and $\\text{diag}(\\cdot)$ stores the components of a vector as the diagonal entries of a matrix. -->"
      ],
      "metadata": {
        "id": "Tb7uAGJ26nG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class simpleHessQuikLayerV1(lay.hessQuikLayer):\n",
        "\n",
        "  def __init__(self, in_features, out_features, act=act.tanhActivation(), \n",
        "               device=None, dtype=None):\n",
        "      factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "      super(simpleHessQuikLayerV1, self).__init__()\n",
        "      self.in_features = in_features\n",
        "      self.out_features = out_features\n",
        "      self.act = act\n",
        "      self.K = nn.Parameter(torch.randn(in_features, out_features, **factory_kwargs))\n",
        "\n",
        "  def dim_input(self):\n",
        "    return self.in_features\n",
        "\n",
        "  def dim_output(self):\n",
        "    return self.out_features\n",
        "\n",
        "  def forward(self, u, do_gradient=False, do_Hessian=False, forward_mode=True, \n",
        "              dudx=None, d2ud2x=None):\n",
        "    (dfdx, d2fd2x) = (None, None)\n",
        "    f, dsig, d2sig = self.act(u @ self.K, do_gradient=do_gradient, do_Hessian=do_Hessian)\n",
        "    \n",
        "    if (do_gradient or do_Hessian):\n",
        "      dfdx = dsig.unsqueeze(1) * self.K\n",
        "\n",
        "      if do_Hessian:\n",
        "        d2fd2x = (d2sig.unsqueeze(1) * self.K).unsqueeze(2) * self.K.unsqueeze(0).unsqueeze(0)\n",
        "        \n",
        "        if d2ud2x is not None:\n",
        "          # Gauss-Newton approximation\n",
        "          d2fd2x = dudx.unsqueeze(1) @ (d2fd2x.permute(0, 3, 1, 2) @ dudx.permute(0, 2, 1).unsqueeze(1))\n",
        "          d2fd2x = d2fd2x.permute(0, 2, 3, 1)\n",
        "          \n",
        "          # extra term to compute full Hessian\n",
        "          d2fd2x += d2ud2x @ dfdx.unsqueeze(1)\n",
        "\n",
        "    if dudx is not None:\n",
        "      dfdx = dudx @ dfdx\n",
        "\n",
        "    return f, dfdx, d2fd2x\n"
      ],
      "metadata": {
        "id": "9U1Uo9u5q-uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing ```hessQuik``` Layers and Networks\n",
        "\n",
        "We use a Taylor series approximation to test new layers.  Suppose we have a ```hessQuik``` network $f$ and we have pre-computed the value and derivatives at a particular point $\\mathbf{x}$ via\n",
        "\n",
        "```python\n",
        "f0, df0, d2f0 = f(x, do_gradient=True, do_Hessian=True)\n",
        "```\n",
        "If our gradient is correct, then if we perturb our input $\\mathbf{x}$ by unit vector $\\mathbf{p}$ with step size $h$, then by Taylor's Theorem,\n",
        "\\begin{align}\n",
        "\\left\\|f(\\mathbf{x}) + h \\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}^\\top \\mathbf{p} - f(\\mathbf{x} + h\\mathbf{p})\\right\\| = \\mathcal{O}(h^2).\n",
        "\\end{align}\n",
        "If we have computed the correct gradient, $\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}$, then as $h\\to 0$, the first-order error (left-hand side) also goes to zero at a rate of $h^2$.  This means if we divide $h$ by $2$ (smaller perturbation), then we should see the first-order error decrease by a factor of $4$. Similar logic can be applied for the zeroth-order (no derivative information) and second-order (Hessian information) error.\n",
        "\n",
        "In the ```input_derivative_check```, we should see the printouts where the first column, ```h```, is continually cut in half. The second column, ```E0```, is the zeroth-order error and is cut in half with each change of ```h```. The third column, ```E1```, is the first-order  error and is divided by 4 with each change of ```h```.  The last column, ```E2```, is the second-order error and is divded by 8 every time ```h``` is halved.  We want to see this behavior consistently, but some steps will not match perfectly due to the numerical approximation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uo8MC--Usrrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hessQuik.utils import input_derivative_check, input_derivative_check_finite_difference\n",
        "\n",
        "my_layer = simpleHessQuikLayerV1(3, 4).to(device)\n",
        "\n",
        "nex = 10\n",
        "x = torch.randn(nex, my_layer.dim_input(), device=device)\n",
        "\n",
        "grad_check, hess_check = input_derivative_check(my_layer, x, do_Hessian=True, verbose=True, forward_mode=True)\n",
        "\n",
        "# an alternative, but typically slower test using finite differences\n",
        "# grad_check, hess_check = input_derivative_check_finite_difference(my_layer, x, do_Hessian=True, verbose=True, forward_mode=True)"
      ],
      "metadata": {
        "id": "2PxcH_RwsrOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same test to verify that a network made from our layers computed derivatives correctly."
      ],
      "metadata": {
        "id": "vddiOueKu6a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hessQuik.networks as net\n",
        "\n",
        "my_networkV1 = net.NN(simpleHessQuikLayerV1(3, 4), \n",
        "                    simpleHessQuikLayerV1(4, 5)).to(device)\n",
        "\n",
        "nex = 10\n",
        "x = torch.randn(nex, my_networkV1.dim_input(), device=device)\n",
        "\n",
        "grad_check, hess_check = input_derivative_check(my_networkV1, x, do_Hessian=True, verbose=True, forward_mode=True)"
      ],
      "metadata": {
        "id": "AJKzXF15thpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative Methods of Computing Derivatives\n",
        "There are many ways to utilize broadcasting and other PyTorch parallelism to make our computation more time and/or storage efficient.  The example above is the method we use in our package, but we provide some alternative options here for completeness.  In certain settings (e.g., CPU vs. GPU, network architecture), one option may be better than the other.  \n",
        "\n",
        "Note: for this simple layer, there is not much difference between the various methods."
      ],
      "metadata": {
        "id": "WV6NHxQVvU6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class simpleHessQuikLayerV2(simpleHessQuikLayerV1):\n",
        "  def __init__(self, in_features, out_features):\n",
        "      super().__init__(in_features, out_features)\n",
        "\n",
        "  def forward(self, u, do_gradient=False, do_Hessian=False, forward_mode=True, \n",
        "              dudx=None, d2ud2x=None):\n",
        "    (dfdx, d2fd2x) = (None, None)\n",
        "    f, dsig, d2sig = self.act(u @ self.K, do_gradient=do_gradient, do_Hessian=do_Hessian)\n",
        "    \n",
        "    if (do_gradient or do_Hessian):\n",
        "      dfdx = dsig.unsqueeze(1) * self.K\n",
        "\n",
        "      if do_Hessian:\n",
        "        # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
        "        # alternative to\n",
        "        # d2fd2x = (d2sig.unsqueeze(1) * self.K).unsqueeze(2) * self.K.unsqueeze(0).unsqueeze(0)\n",
        "        d2fd2x = (d2sig.unsqueeze(-1).unsqueeze(-1) * (self.K.T.unsqueeze(-1) @ self.K.T.unsqueeze(1)))\n",
        "        d2fd2x = d2fd2x.permute(0, 2, 3, 1)\n",
        "        # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
        "\n",
        "        if d2ud2x is not None:\n",
        "          # Gauss-Newton approximation\n",
        "          d2fd2x = dudx.unsqueeze(1) @ (d2fd2x.permute(0, 3, 1, 2) @ dudx.permute(0, 2, 1).unsqueeze(1))\n",
        "          d2fd2x = d2fd2x.permute(0, 2, 3, 1)\n",
        "          \n",
        "          # extra term to compute full Hessian\n",
        "          d2fd2x += d2ud2x @ dfdx.unsqueeze(1)\n",
        "\n",
        "    if dudx is not None:\n",
        "      dfdx = dudx @ dfdx\n",
        "    return f, dfdx, d2fd2x\n",
        "\n",
        "\n",
        "my_networkV2 = net.NN(simpleHessQuikLayerV2(3, 4), \n",
        "                      simpleHessQuikLayerV2(4, 5)).to(device)\n",
        "\n",
        "nex = 10\n",
        "x = torch.randn(nex, my_networkV2.dim_input(), device=device)\n",
        "\n",
        "grad_check, hess_check = input_derivative_check(my_networkV2, x, do_Hessian=True, verbose=True, forward_mode=True)"
      ],
      "metadata": {
        "id": "mDqQHHVSvWXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class simpleHessQuikLayerV3(simpleHessQuikLayerV1):\n",
        "  def __init__(self, in_features, out_features):\n",
        "      super().__init__(in_features, out_features)\n",
        "\n",
        "  def forward(self, u, do_gradient=False, do_Hessian=False, forward_mode=True, \n",
        "              dudx=None, d2ud2x=None):\n",
        "    (dfdx, d2fd2x) = (None, None)\n",
        "    f, dsig, d2sig = self.act(u @ self.K, do_gradient=do_gradient, do_Hessian=do_Hessian)\n",
        "    \n",
        "    if (do_gradient or do_Hessian):\n",
        "      dfdx = dsig.unsqueeze(1) * self.K\n",
        "\n",
        "      if do_Hessian:\n",
        "        d2fd2x = (d2sig.unsqueeze(1) * self.K).unsqueeze(2) * self.K.unsqueeze(0).unsqueeze(0)\n",
        "        \n",
        "        if d2ud2x is not None:\n",
        "          # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
        "          # alternative to\n",
        "          # # Gauss-Newton approximation\n",
        "          # d2fd2x = dudx.unsqueeze(1) @ (d2fd2x.permute(0, 3, 1, 2) @ dudx.permute(0, 2, 1).unsqueeze(1))\n",
        "          # d2fd2x = d2fd2x.permute(0, 2, 3, 1)\n",
        "          \n",
        "          # # extra term to compute full Hessian\n",
        "          # d2fd2x += d2ud2x @ dfdx.unsqueeze(1)\n",
        "\n",
        "          # Gauss-Newton approximation\n",
        "          d2fd2x = torch.sum(dudx.unsqueeze(2).unsqueeze(-1) * d2fd2x.unsqueeze(1), dim=3)\n",
        "          d2fd2x = torch.sum(dudx.unsqueeze(2).unsqueeze(-1) * d2fd2x.unsqueeze(1), dim=3)\n",
        "          \n",
        "          # extra term to compute full Hessian\n",
        "          d2fd2x += torch.sum(dfdx.unsqueeze(1).unsqueeze(1) * d2ud2x.unsqueeze(4), dim=3)\n",
        "          # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
        "\n",
        "    if dudx is not None:\n",
        "      dfdx = dudx @ dfdx\n",
        "\n",
        "    return f, dfdx, d2fd2x\n",
        "\n",
        "\n",
        "my_networkV3 = net.NN(simpleHessQuikLayerV3(3, 4), \n",
        "                      simpleHessQuikLayerV3(4, 5)).to(device)\n",
        "\n",
        "nex = 10\n",
        "x = torch.randn(nex, my_networkV2.dim_input(), device=device)\n",
        "\n",
        "grad_check, hess_check = input_derivative_check(my_networkV3, x, do_Hessian=True, verbose=True, forward_mode=True)"
      ],
      "metadata": {
        "id": "97d--_3FwgEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computational Efficiency of Derivative Computation\n",
        "\n",
        "Let's compare the timing of computing the derivative when constructing networks from these various layers.  Feel free to play with different network parameters."
      ],
      "metadata": {
        "id": "y_n5tAeHxY83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# parameters to play with\n",
        "in_features = 2     # number of input featurs\n",
        "width = 10          # width of network\n",
        "depth = 4           # number of hidden layers\n",
        "out_features = 1    # number of output features\n",
        "nex = 10            # number of examples\n",
        "\n",
        "\n",
        "# helper functions\n",
        "def create_network(layer, in_features, out_features, width, depth):\n",
        "\n",
        "  args = (layer(in_features, width),)\n",
        "  for _ in range(depth):\n",
        "    args += (layer(width, width),)\n",
        "  args += (layer(width, out_features),)\n",
        "\n",
        "  test_net = net.NN(*args)\n",
        "  return test_net\n",
        "\n",
        "\n",
        "def run_test(test_net, x, num_trials=10):\n",
        "  total_time = 0.0\n",
        "  for _ in range(num_trials):\n",
        "    t1_start = time.time()\n",
        "    f0, df0, d2f0 = test_net(x, do_gradient=True, do_Hessian=True, forward_mode=True)\n",
        "    t1_end = time.time()\n",
        "    total_time += t1_end - t1_start\n",
        "  \n",
        "  return total_time / num_trials\n",
        "\n",
        "\n",
        "# inputs\n",
        "x = torch.randn(nex, in_features, device=device)\n",
        "\n",
        "# timing tests\n",
        "test_net = create_network(simpleHessQuikLayerV1, in_features, out_features, width, depth)\n",
        "test_net = test_net.to(device)\n",
        "time1 = run_test(test_net, x)\n",
        "print('Implementation 1: %0.4f' % time1)\n",
        "\n",
        "test_net = create_network(simpleHessQuikLayerV2, in_features, out_features, width, depth).to(device)\n",
        "test_net = test_net.to(device)\n",
        "time2 = run_test(test_net, x)\n",
        "print('Implementation 2: %0.4f' % time2)\n",
        "\n",
        "test_net = create_network(simpleHessQuikLayerV3, in_features, out_features, width, depth).to(device)\n",
        "test_net = test_net.to(device)\n",
        "time3 = run_test(test_net, x)\n",
        "print('Implementation 3: %0.4f' % time3)"
      ],
      "metadata": {
        "id": "nodKcSPQxdYZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}