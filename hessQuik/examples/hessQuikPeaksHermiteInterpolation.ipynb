{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hessQuikPeaksHermiteInterpolation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Hermite Interpolation using ```hessQuik```\n",
        "\n",
        "[Hermite interpolation](https://en.wikipedia.org/wiki/Hermite_interpolation) is a function approximation technique that incorporates derivative information.  Instead of having only the function values at a set of points, we have the function values and the values of the derivatives up to some order (in our case, order-2).  Ideally, we can obtain a quality approximation over the whole space using fewer points because each point contains additional information.\n",
        "\n",
        "We approximate a PyTorch implementation of [MATLAB's peaks function](https://www.mathworks.com/help/matlab/ref/peaks.html).  We additionally provide code to compute the derivative and Hessian of the peaks function.  To find details about the peaks function and training algorithm, see the [data and training utilities](https://github.com/elizabethnewman/hessQuik/tree/main/hessQuik/utils) in the repository."
      ],
      "metadata": {
        "id": "Uh8Ixnod1nd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install ```hessQuik``` and Other Packages\n",
        "You can use pip to install ```hessQuik```.  Be sure to reset your Google Colab and reinstall if changes to the repository are made."
      ],
      "metadata": {
        "id": "opJd4Bu5nVOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fzFfJX2FaaT"
      },
      "outputs": [],
      "source": [
        "!python -m pip install git+https://github.com/elizabethnewman/hessQuik.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import hessQuik.activations as act\n",
        "import hessQuik.layers as lay\n",
        "import hessQuik.networks as net\n",
        "from hessQuik.utils import peaks, train_one_epoch, test, print_headers\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)"
      ],
      "metadata": {
        "id": "4u2xAKPLm17Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Training, Validation, and Test Data\n",
        "\n",
        "Create a training, validation, and test set."
      ],
      "metadata": {
        "id": "lDhcL78wm7Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = 1000      # number of training points\n",
        "n_val = 100         # number of validation points\n",
        "n_test = 100        # number of testing points\n",
        "\n",
        "# generate data\n",
        "x = -3 + 6 * torch.rand(n_train + n_val + n_test, 2, device=device)\n",
        "yc, dy, d2y = peaks(x, do_gradient=True, do_Hessian=True)\n",
        "y = torch.cat((yc, dy.view(yc.shape[0], -1), d2y.view(yc.shape[0], -1)), dim=1)\n",
        "# y = {'f': yc, 'df': dy, 'd2f': d2y}\n",
        "\n",
        "# shuffle and split data\n",
        "idx = torch.randperm(n_train + n_val + n_test)\n",
        "x_train, y_train = x[idx[:n_train]], y[idx[:n_train]]\n",
        "x_val, y_val = x[idx[n_train:n_train + n_val]], y[idx[n_train:n_train + n_val]]\n",
        "x_test, y_test = x[idx[n_train + n_val:]], y[idx[n_train + n_val:]]\n"
      ],
      "metadata": {
        "id": "Ns3rWf-ulxgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Network and Optimizer\n",
        "\n",
        "Build your own ```hessQuik``` architecture!"
      ],
      "metadata": {
        "id": "QaBeb5ZXnDLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "width = 16\n",
        "depth = 4\n",
        "f = net.NN(lay.singleLayer(2, width, act=act.tanhActivation()),\n",
        "           net.resnetNN(width, depth, h=0.5, act=act.tanhActivation()),\n",
        "           lay.singleLayer(width, 1, act=act.identityActivation())).to(device)\n",
        "\n",
        "# Pytorch optimizer for the network weights\n",
        "optimizer = torch.optim.Adam(f.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "Wy9bDCPsO3dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train!"
      ],
      "metadata": {
        "id": "i1D4bvYunInX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training parameters\n",
        "max_epochs = 50\n",
        "batch_size = 5\n",
        "loss_weights = (1.0, 1.0, 1.0)\n",
        "do_gradient = True\n",
        "do_Hessian = True\n",
        "\n",
        "# get printouts\n",
        "headers, printouts_str, printouts_frmt = print_headers(do_gradient=do_gradient, do_Hessian=do_Hessian, loss_weights=loss_weights)\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# initial evaluation\n",
        "loss_train = test(f, x_train, y_train, do_gradient=do_gradient, do_Hessian=do_Hessian, loss_weights=loss_weights)\n",
        "loss_val = test(f, x_val, y_val, do_gradient=do_gradient, do_Hessian=do_Hessian, loss_weights=loss_weights)\n",
        "\n",
        "n_loss = 2 + do_gradient + do_Hessian\n",
        "his_iter = (-1, 0.0) + ('|',) + (n_loss * (0,)) + ('|',) + loss_train + ('|',) + loss_val\n",
        "print(printouts_frmt.format(*his_iter))\n",
        "\n",
        "# store history\n",
        "his = np.array([x for x in his_iter if not (x == '|')]).reshape(1, -1)\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# main iteration\n",
        "\n",
        "log_interval = 5 # how often printouts appear\n",
        "for epoch in range(max_epochs):\n",
        "    t0 = time.perf_counter()\n",
        "    running_loss = train_one_epoch(f, x_train, y_train, optimizer, batch_size,\n",
        "                                   do_gradient=do_gradient, do_Hessian=do_Hessian, loss_weights=loss_weights)\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    # test\n",
        "    loss_train = test(f, x_train, y_train, do_gradient=do_gradient, do_Hessian=do_Hessian, loss_weights=loss_weights)\n",
        "    loss_val = test(f, x_val, y_val, do_gradient=do_gradient, do_Hessian=do_Hessian, loss_weights=loss_weights)\n",
        "\n",
        "    his_iter = (epoch, t1 - t0) + ('|',) + running_loss + ('|',) + loss_train + ('|',) + loss_val\n",
        "\n",
        "    if epoch % log_interval == 0:\n",
        "      print(printouts_frmt.format(*his_iter))\n",
        "\n",
        "    # store history\n",
        "    his = np.concatenate((his, np.array([x for x in his_iter if not (x == '|')]).reshape(1, -1)), axis=0)\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# overall performance on test data\n",
        "loss_test = test(f, x_test, y_test, do_gradient=do_gradient, do_Hessian=do_Hessian, loss_weights=loss_weights)\n",
        "print('Test Loss: %0.4e' % loss_test[0])"
      ],
      "metadata": {
        "id": "2C-BrG50mF5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize!\n",
        "\n",
        "We show the quality peaks approximation as an image and we plot the convergence of the three losses, function value, gradient, and Hessian."
      ],
      "metadata": {
        "id": "cPBBci3d8k8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "xy = torch.arange(-3, 3, 0.25, dtype=torch.float32)\n",
        "grid_x, grid_y = torch.meshgrid(xy, xy, indexing='xy')\n",
        "\n",
        "grid_data = torch.cat((grid_x.reshape(-1, 1), grid_y.reshape(-1, 1)), dim=1)\n",
        "grid_data = grid_data.to(device)\n",
        "\n",
        "# approximation\n",
        "c_true = peaks(grid_data)[0].view(grid_x.shape).cpu().numpy()\n",
        "c_pred = f.forward(grid_data)[0].detach().view(grid_x.shape).cpu().numpy()\n",
        "\n",
        "print('relative error ||c_approx - c_true|| / ||c_true|| = ',\n",
        "      np.linalg.norm(c_pred.reshape(-1) - c_true.reshape(-1)) / np.linalg.norm(c_true.reshape(-1)), '\\n')\n",
        "\n",
        "# image plots\n",
        "fig, axs = plt.subplots(2, 2);\n",
        "ax = axs[0, 0];\n",
        "p = ax.imshow(c_true.reshape(grid_x.shape));\n",
        "ax.axis('off');\n",
        "ax.set_title('true');\n",
        "fig.colorbar(p, ax=ax, aspect=10);\n",
        "\n",
        "ax = axs[0, 1];\n",
        "p = ax.imshow(c_pred.reshape(grid_x.shape), vmin=c_true.min(), vmax=c_true.max());\n",
        "ax.axis('off');\n",
        "ax.set_title('approx');\n",
        "fig.colorbar(p, ax=ax, aspect=10);\n",
        "\n",
        "ax = axs[1, 0];\n",
        "p = ax.imshow(np.abs(c_pred - c_true).reshape(grid_x.shape));\n",
        "fig.colorbar(p, ax=ax, aspect=10);\n",
        "ax.axis('off');\n",
        "ax.set_title('abs. diff.');\n",
        "\n",
        "ax = axs[1, 1];\n",
        "ax.axis('off');\n",
        "plt.show()\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# convergence plots\n",
        "plt.figure()\n",
        "linewidth = 3\n",
        "idx = [idx for idx, n in enumerate(np.array([x for x in printouts_str if not (x == '|')])) if n == 'loss_f'][1]\n",
        "\n",
        "plt.semilogy(his[:, 0], his[:, idx], linewidth=linewidth, label='f')\n",
        "\n",
        "if do_gradient: \n",
        "  plt.semilogy(his[:, 0], his[:, idx + 1], linewidth=linewidth, label='df')\n",
        "\n",
        "if do_Hessian:  \n",
        "  plt.semilogy(his[:, 0], his[:, idx + 2], linewidth=linewidth, label='d2f')\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('training loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_5MuP3axba-B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}